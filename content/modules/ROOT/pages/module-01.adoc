= Using Red Hat AI Inference Server

In this lab, you will be using Red Hat AI Inference Server, running in a container on RHEL 9.5 and serving the Llama-3-Groq-8B model.
In order to get it the inference server running you need to pull its iamge from the registry and tehn create and run a container wit that imag. To save you time, the image that will be used in the container has been pre-pulled and it is already available in your RHEL host, that has an NVIDIA L4 GPU with 24Gb of memory. As we said in the introduction, Red Hat AI Inference Server can use many other accelerators. The container as well has been created and it is running in the host.


== Monitor GPU utilization

In one of the terminals we are going to check the GPU utilization by typing this command:

+
[source,bash]
----
nvtop
----
+ 
NOTE: Initially the GPU utilization is 0 as there is no load in it.


== Query Red Hat AI Inference Server

In the other terminal we will submit a query using the *curl* command:


+
[source,bash]
----
curl -X POST http://{url}:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Groq/Llama-3-Groq-8B-Tool-Use",
    "messages": [
      {
        "role": "user",
        "content": "Write me 10 to 15 paragraphs about RHEL"
      }
    ],
    "temperature": 0.7,
    "max_tokens": 5000
  }' | jq

----
+

Look at the termial where the GPU monitor is running and you will see how its resources are being used until the response is displayed.

=== Using OpenAPI

In this section we will use OpenAI API to query Red Hat AI Inference Server, which is really useful if we want applications to communicate with it. OpenAI API has become a standard as it  supports stateful interactions, function calling, and integration with external tools and data, making it a powerful resource for building intelligent applications.

We will create a Python script that will submit the query to the Red Hat AI Inference Server. But before we need to install the OpenAI library.

+
[source,bash]
----
pip install openai
----
+ 

Now let's create the script using the *vim* editor.

+
[source,bash]
----
vim api.py
----
+ 

Once inside the *vim* editor press '*i*' to enter edit mode and paste this code snippet:


+
[source,bash]
----
from openai import OpenAI

api_key = "llamastack"



model = "Groq/Llama-3-Groq-8B-Tool-Use"
base_url = "http://{url}:8000/v1/"


client = OpenAI(
    base_url=base_url,
    api_key=api_key,
)

response = client.chat.completions.create(
    model=model,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Why is Red Hat AI Inference Server a great fit for RHEL?"}
    ]
)
print(response.choices[0].message.content)

----
+

Again we can see in the GPU monitor how its resources are being utilized until the response is returned.

You can experiment with different queries by just modifying the question in both the Python script or the *curl* command.