= Using Red Hat AI Inference Server

In this lab, you will be using Red Hat AI Inference Server, running in a container on RHEL 9.5 and serving the Llama-3-Groq-8B model.
In order to get the inference server running you would need to pull its image from the registry and then create and run a container with that image.

To save you time, the image that will be used in the container has been pre-pulled and is already available in your RHEL host.
The RHEL server has, by default, an NVIDIA L4 GPU with 24Gb of memory. 
As we said in the introduction, Red Hat AI Inference Server can use many other accelerators. 
The RHAIIS container has been created and it is already running on the host.


== Monitor GPU utilization

In the terminal on top, check the GPU utilization by typing this command:

[source,yaml,role=execute,subs=attributes+]
----
nvtop
----
 
NOTE: Initially the GPU utilization is 0 as there is no load in it.

== Query Red Hat AI Inference Server

In the bottom terminal, submit this query using the *curl* command:

[source,yaml,role=execute,subs=attributes+]
----
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Groq/Llama-3-Groq-8B-Tool-Use",
    "messages": [
      {
        "role": "user",
        "content": "Write me 10 to 15 paragraphs about RHEL"
      }
    ],
    "temperature": 0.7,
    "max_tokens": 5000
  }' | jq
----

Look at the termial where the GPU monitor is running and you will see how its resources are being used until the response is displayed.

Try a few more queries by adjusting the content in the API call above and see how the GPU resources are used.

=== Using OpenAI API

Now, you will use OpenAI API to query Red Hat AI Inference Server.
This is really useful if we want applications to be able to communicate with it. 
OpenAI API has become a standard as it supports stateful interactions, function calling, and integration with external tools and data, making it a powerful resource for building intelligent applications.

We will create a Python script that will submit the query to the Red Hat AI Inference Server. But before we need to install the OpenAI library.

[source,yaml,role=execute,subs=attributes+]
----
pip install openai
----

As you can see in the simple script below, it is creating a client that asks our model why Red Hat AI Inference Server is good for RHEL.

You will need to generate an API key so that the script can use OpenAI API. Follow https://www.guidingtech.com/how-to-generate-openai-api-key/[these instructions]
[this document] to generate your key, then substitute '<your_api_key>' in the script with its value.

[source,yaml,role=execute,subs=attributes+]
----
cat << 'EOF' > api.py
from openai import OpenAI

api_key = "llamastack"

model = "Groq/Llama-3-Groq-8B-Tool-Use"
base_url = "http://localhost:8000/v1/"

client = OpenAI(
    base_url=base_url,
    api_key=<your_api_key>,
)

response = client.chat.completions.create(
    model=model,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Why is Red Hat AI Inference Server a great fit for RHEL?"}
    ]
)
print(response.choices[0].message.content)
EOF
----

Once again, you can see in the GPU monitor how its resources are being utilized until the response is returned.

Try a few more queries by adjusing the content in the Python script or the *curl* command to see how the GPU resources are used and the accuracy of the responses.
