= Using Red Hat AI Inference Server

In this lab, you will be using Red Hat AI Inference Server, running in a container on RHEL 9.5 and serving the Llama-3-Groq-8B model.
In order to get it the inference server running you would need to pull its image from the registry and then create and run a container with that image.

To save you time, the image that will be used in the container has been pre-pulled and is already available in your RHEL host.
The RHEL server has, by default, an NVIDIA L4 GPU with 24Gb of memory. 
As we said in the introduction, Red Hat AI Inference Server can use many other accelerators. 
The RHAIIS container has been created and it is already running on the host.


== Monitor GPU utilization

In the terminal on top, check the GPU utilization by typing this command:

[source,yaml,role=execute,subs=attributes+]
----
nvtop
----
 
NOTE: Initially the GPU utilization is 0 as there is no load in it.

== Query Red Hat AI Inference Server

In the bottom terminal, submit this query using the *curl* command:

[source,yaml,role=execute,subs=attributes+]
----
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Groq/Llama-3-Groq-8B-Tool-Use",
    "messages": [
      {
        "role": "user",
        "content": "Write me 10 to 15 paragraphs about RHEL"
      }
    ],
    "temperature": 0.7,
    "max_tokens": 5000
  }' | jq
----

Look at the termial where the GPU monitor is running and you will see how its resources are being used until the response is displayed.

Try a few more queries by adjusing the content in the API call above and see how the GPU resources are used.

=== Using OpenAPI

Now, you will use OpenAI API to query Red Hat AI Inference Server.
This is really useful if we want applications to be able to communicate with it. 
OpenAI API has become a standard as it supports stateful interactions, function calling, and integration with external tools and data, making it a powerful resource for building intelligent applications.

We will create a Python script that will submit the query to the Red Hat AI Inference Server. But before we need to install the OpenAI library.

[source,yaml,role=execute,subs=attributes+]
----
pip install openai
----

As you can see in the simple script below, it is doing TODO.
It is using the API key to authenticate to the TODO

[source,yaml,role=execute,subs=attributes+]
----
cat << 'EOF' > api.py
from openai import OpenAI

api_key = "llamastack"

model = "Groq/Llama-3-Groq-8B-Tool-Use"
base_url = "http://localhost:8000/v1/"

client = OpenAI(
    base_url=base_url,
    api_key=api_key,
)

response = client.chat.completions.create(
    model=model,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Why is Red Hat AI Inference Server a great fit for RHEL?"}
    ]
)
print(response.choices[0].message.content)
EOF
----

Once again, you can see in the GPU monitor how its resources are being utilized until the response is returned.

Try a few more queries by adjusing the content in the Python script or the *curl* command to see how the GPU resources are used.
